import streamlit as st 
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score
from sklearn.model_selection import KFold, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

import ydata_profiling as yp
import webbrowser

# TODO: Fix the storage of self.best_param as a dictionary and correct the process of finding the best parameters
# TODO: Optimize the parameter search ranges for the models
# TODO: Add a new dataset
# TODO: Conduct a more in-depth Exploratory Data Analysis (EDA) for Breast Cancer dataset

class App:

    def __init__(self):
        self.data = None
        self.dataset_name = None 
        self.classifier_name = None

        self.params = dict()
        self.clf = None
        self.X, self.y = None, None
        self.best_param = None

        self.Init_Streamlit_Page()
   
    def run(self):
        self.get_dataset()
        self.data_preprocess_breast_cancer()
        self.parameter_tuning()
        self.add_parameter_ui()
        self.models()
    
    def Init_Streamlit_Page(self):

        self.dataset_name = st.sidebar.selectbox(
            "Select Dataset", 
            ("Breast Cancer",)) 
        
        st.title(f"{self.dataset_name} Analysis")


        st.expander("Dataset information" ).write("""
                                                The "Diagnostic Wisconsin Breast Cancer Database" is a publicly available data set from the UCI machine learning repository.
                                                The dataset gives information about tumor features, that are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.
                                                For each observation there are 10 features, which describe tumor size, density, texture, symmetry, and other characteristics of the cell nuclei present in the image.
                                                The mean, standard error and "worst" mean (mean of the three largest values) of these features were computed for each image, resulting in 30 features.
                                                The categorical target feature indicates the type of the tumor.
                                                The area on the aspirate slides to be analyzed was visually selected for minimal nuclear overlap.
                                                The image for digital analysis was generated by a JVC TK-1070U color video camera mounted above an Olympus microscope and 
                                                the image was projected into the camera with a 63 x objective and a 2.5 x ocular.
                                                The image was captured as a 512 x 480 resolution, 8 bit/pixel (Black and White) file.
                                                The aspirated material was expressed onto a silane-coated glass slide, which was placed under a similar slide.
                                                A typical image contains approximately from 10 to 40 nuclei. After computing 10 features for each nucleus, the mean, standart error and extreme value was computed, as it mentioned above.
                                                These features are modeled such that higher values are typically associated with malignancy.
                                                  """)
        
        
        
        st.write(f"## {self.dataset_name} Dataset")

        self.classifier_name = st.sidebar.selectbox(
            "Select Classifier",
            ("KNN", "SVM", "Naive Bayes(GaussianNB)")
        )

    ### dataset can not be cached problem with self keyword occurs 
    def get_dataset(self):
        if self.dataset_name == "Breast Cancer":
            try:
                self.data = pd.read_csv("data.csv")
                #success_message = st.success("Dataset loaded successfully")
                with st.status("Downloading data...", expanded=True) as status:
                    st.write("Searching for data...")
                    st.write("Found URL.")
                    st.write("Downloading data...")
                    status.update(label=f"{self.dataset_name} Dataset download complete!", state="complete", expanded=False)
                self.data_intro()    
            except Exception as e:
                st.write(f"An error occurred: {e}")

        
    @st.cache_data
    def generate_report(_self):
        report = yp.ProfileReport(_self.data, explorative=True, minimal=True)
        report.to_file(f"{_self.dataset_name} Report.html")
        

    def data_intro(self):
        report_button = st.button("Generate Dataset Variable Report")
        if(report_button):
            self.generate_report()
            webbrowser.open(f"{self.dataset_name} Report.html")
        st.write("Dataframe first 10 rows: ", self.data.head(10))
        st.write("Shape of Dataset: ", self.data.shape, "  *This dataset consists of 569 samples, each described by 33 features*")
        st.write("Target Value: ", self.data['diagnosis'].value_counts())
        st.write(f"*2 unique values in target column and they are {self.data['diagnosis'].unique()}, their count is {self.data['diagnosis'].value_counts()} respectively*")  

        # Calculate the number of missing values in each column
        missing_values = self.data.isnull().sum()

        # Filter the columns with missing values
        columns_with_missing_values = missing_values[missing_values > 0]

        # Display the columns with missing values and the count of missing values in each column
        if not columns_with_missing_values.empty:
            st.write("Columns with Missing Values:", columns_with_missing_values)
        else:
            st.write("No missing values in the dataset")

        st.write("Categorical Features:" , self.data.dtypes[self.data.dtypes == "object"])
        
    
    def data_preprocess_breast_cancer(self):
        st.subheader("Data Preprocessing")
        st.write(f"***Drop Unnecessary Columns:*** id, Unnamed: 32")
        self.data.drop(["id", "Unnamed: 32"], axis=1, inplace=True)
        st.write(self.data.head(5),"*Missing values handled with dropping Unnamed: 32 column*" )

        st.write(f"***Encoding Label:*** diagnosis - Malignant - 1, Benign - 0")
        self.data['diagnosis'] = self.data['diagnosis'].map({'M': 1, 'B':0})
        st.write(self.data["diagnosis"].tail(10))

        self.X = self.data.drop(["diagnosis"], axis=1)
        self.y = self.data["diagnosis"].values

        self.target_value_corelation_plot()
        self.X = (self.X - self.X.min()) / (self.X.max() - self.X.min())
        st.write("***Normalized Data***: ", self.X.head(10))

    @st.cache_data
    def target_value_corelation_plot(_self):

        tab1, tab2 = st.tabs(["ðŸ—ƒ Matrix", "ðŸ“ˆ Plot"])

        with tab1:
            tab1.subheader("Correlation Matrix ")
            if _self.dataset_name == "Breast Cancer":
                
                # create correlation matrix
                correlation_matrix = _self.data.corr()

                st.write("Correlation Matrix:")
                st.write(correlation_matrix)
                st.write("##### Correlation Matrix Heatmap with Target Values:")
                st.table(correlation_matrix['diagnosis'])
               

        
        with tab2:
            tab2.subheader("Correlation Plot ")

            # Display the correlation matrix heatmap
            st.write("##### Correlation Matrix Heatmap All Features:")

            plt.figure(figsize=(32, 16))
            # Store heatmap object in a variable to easily access it when you want to include more features (such as title).
            # Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.
            heatmap = sns.heatmap(correlation_matrix, vmin=-1, vmax=1, annot=True, cmap='BrBG')
            # Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.
            heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);
            st.pyplot(plt)

            # Display the correlation matrix heatmap with target values
            diagnosis_correlation = correlation_matrix['diagnosis']

            st.write("##### Correlation Matrix Heatmap with Target Values:")
            plt.figure(figsize=(10, 8))
            sns.heatmap(diagnosis_correlation.to_frame(), annot=True, cmap='BrBG', fmt=".2f")
            plt.title('Correlation Matrix Heatmap with Target Values')
            plt.xticks(rotation=45)
            plt.yticks(rotation=0)
            st.pyplot(plt)

     # Display scatter plot
        malignant_data = _self.data[_self.data['diagnosis'] == 1]
        benign_data = _self.data[_self.data['diagnosis'] == 0]
        plt.figure(figsize=(8, 6))
        st.write("***Scatter Plot of Radius Mean vs Texture Mean:***")
        # Make scatter plot transparent with less opacity
        sns.scatterplot(x='radius_mean', y='texture_mean', data=malignant_data, label='Malignant', color='red', alpha=0.5)
        sns.scatterplot(x='radius_mean', y='texture_mean', data=benign_data, label='Benign', color='blue', alpha=0.5)
        plt.title('Scatter Plot of Radius Mean vs Texture Mean')
        plt.xlabel('Radius Mean')
        plt.ylabel('Texture Mean')
        plt.legend()

        st.pyplot(plt)

        #postive correlation
        st.write("***Scatter Plot of positive correlation:***")
        fig,ax=plt.subplots(2,2,figsize=(20,25))
        sns.scatterplot(x='perimeter_mean',y='radius_worst',data=_self.data,hue='diagnosis',ax=ax[0][0])
        sns.scatterplot(x='area_mean',y='radius_worst',data=_self.data,hue='diagnosis',ax=ax[1][0])
        sns.scatterplot(x='texture_mean',y='texture_worst',data=_self.data,hue='diagnosis',ax=ax[0][1])
        sns.scatterplot(x='area_worst',y='radius_worst',data=_self.data,hue='diagnosis',ax=ax[1][1])
        st.pyplot(fig)       

        #negative correlation
        st.write("***Scatter Plot of negative correlation:***")
        fig,ax=plt.subplots(2,2,figsize=(20,25))
        sns.scatterplot(x='area_mean',y='fractal_dimension_mean',data=_self.data,hue='diagnosis',ax=ax[0][0])
        sns.scatterplot(x='radius_mean',y='smoothness_se',data=_self.data,hue='diagnosis',ax=ax[1][0])
        sns.scatterplot(x='smoothness_se',y='perimeter_mean',data=_self.data,hue='diagnosis',ax=ax[0][1])
        sns.scatterplot(x='area_mean',y='smoothness_se',data=_self.data,hue='diagnosis',ax=ax[1][1])
        st.pyplot(fig)

    
    def parameter_tuning(self):
        st.subheader("**Model**")
        st.write(f"***Classifiers*** = {self.classifier_name}")
        
        #### HYPERPARAMETER TUNING ####
        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)

        if self.classifier_name == "KNN":
            kf=KFold(n_splits=5,shuffle=True,random_state=42)
            parameter={'n_neighbors': np.arange(1, 15, 1)}
            knn=KNeighborsClassifier()
            knn_cv=GridSearchCV(knn, param_grid=parameter, cv=kf, verbose=1, scoring='accuracy')
            knn_cv.fit(X_train, y_train)
            st.write("best parameter values: ", knn_cv.best_params_)
            self.best_param = knn_cv.best_params_["n_neighbors"]

        elif self.classifier_name == "SVM":
            param_grid = {'C': np.arange(1, 10, 1), 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'linear']}
            grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,scoring='accuracy')
            grid.fit(X_train, y_train)
            st.write("best parameter values: ", grid.best_params_)
            self.best_param = grid.best_params_["C"]    

        elif self.classifier_name == "Naive Bayes(GaussianNB)":
            params_NB = {'var_smoothing': np.arange(0.1,2.0)}
            gs_NB = GridSearchCV(estimator=GaussianNB(), 
                            param_grid=params_NB, 
                            cv=KFold(n_splits=5),
                            verbose=1, 
                            scoring='accuracy') 
            gs_NB.fit(X_train, y_train)
            st.write("best parameter values: ", gs_NB.best_params_)
            self.best_param = gs_NB.best_params_["var_smoothing"]


    def add_parameter_ui(self):
        if self.classifier_name == "SVM":
            C = st.sidebar.slider("C", 1, 10,step=1, value= self.best_param)
            self.params["C"] = C
            gamma = st.sidebar.select_slider("gamma", options =[1,0.1,0.01,0.001], value= self.best_param)
            self.params["gamma"] = gamma
            kernel = st.sidebar.radio("kernel", ("rbf", "poly", "linear")) 
            self.params["kernel"] = kernel

        elif self.classifier_name == "KNN":
            n_neighbors = st.sidebar.slider("n_neighbors", 1, 15, value= self.best_param)
            self.params["n_neighbors"] = n_neighbors

        elif self.classifier_name == "Naive Bayes(GaussianNB)":
            var_smoothing = st.sidebar.slider("var_smoothing", 0.01, 5.0, value= self.best_param)
            self.params["var_smoothing"] = var_smoothing
        


    def models(self):
        self.get_classifier()
        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)

        st.write("parameters you choose: ", self.params)
        

        self.clf.fit(X_train, y_train)
        y_pred = self.clf.predict(X_test)

        acc = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)

        st.write(f"Accuracy = {acc}")
        st.write(f"F1 Score = {f1}")
        st.write(f"Precision = {precision}")
        st.write(f"Recall = {recall}")

        # Define the categories for confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        st.write("***Confusion Matrix:***")
        st.write(cm)
        
        #plot 
        f, ax =plt.subplots(figsize = (5,5))

        sns.heatmap(cm,annot = True, linewidths= 0.5, linecolor="red", fmt=".0f", ax=ax)
        plt.xlabel("y_pred")
        plt.ylabel("y_true")
        st.pyplot(f)  

    
    def get_classifier(self):
        if self.classifier_name == "KNN":
            self.clf = KNeighborsClassifier(n_neighbors=self.params["n_neighbors"])
        
        elif self.classifier_name == "SVM":
            self.clf = SVC(C=self.params["C"], kernel=self.params["kernel"],gamma=self.params["gamma"])
        
        
        elif self.classifier_name == "Naive Bayes(GaussianNB)":
            self.clf = GaussianNB(var_smoothing=self.params["var_smoothing"])