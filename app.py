import streamlit as st 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
import pandas as pd


from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score
from sklearn.model_selection import KFold, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

import ydata_profiling as pp
import streamlit.components.v1 as components


class App:

    def __init__(self):
        self.data = None
        self.dataset_name = None 
        self.classifier_name = None
        

        self.params = dict()
        self.clf = None
        self.X, self.y = None, None
        self.best_param = None

        self.cache = st.session_state.get('cache', {})
        self.Init_Streamlit_Page()
   
    def run(self):
        self.get_dataset()
        self.data_preprocess_breast_cancer()
        self.parameter_tuning()
        self.add_parameter_ui()
        self.models()
    
    def Init_Streamlit_Page(self):

        self.dataset_name = st.sidebar.selectbox(
            "Select Dataset", 
            ("Breast Cancer",)) 
        
        st.title(f"{self.dataset_name} Analysis")


        st.expander("Dataset information" ).write("""
                                                The "Diagnostic Wisconsin Breast Cancer Database" is a publicly available data set from the UCI machine learning repository.
                                                The dataset gives information about tumor features, that are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.
                                                For each observation there are 10 features, which describe tumor size, density, texture, symmetry, and other characteristics of the cell nuclei present in the image.
                                                The mean, standard error and "worst" mean (mean of the three largest values) of these features were computed for each image, resulting in 30 features.
                                                The categorical target feature indicates the type of the tumor.
                                                The area on the aspirate slides to be analyzed was visually selected for minimal nuclear overlap.
                                                The image for digital analysis was generated by a JVC TK-1070U color video camera mounted above an Olympus microscope and 
                                                the image was projected into the camera with a 63 x objective and a 2.5 x ocular.
                                                The image was captured as a 512 x 480 resolution, 8 bit/pixel (Black and White) file.
                                                The aspirated material was expressed onto a silane-coated glass slide, which was placed under a similar slide.
                                                A typical image contains approximately from 10 to 40 nuclei. After computing 10 features for each nucleus, the mean, standart error and extreme value was computed, as it mentioned above.
                                                These features are modeled such that higher values are typically associated with malignancy.
                                                  """)
        
        
        
        st.write(f"## {self.dataset_name} Dataset")

        self.classifier_name = st.sidebar.selectbox(
            "Select Classifier",
            ("KNN", "SVM", "Naive Bayes(GaussianNB)")
        )

   
    def get_dataset(self):
        
        if self.dataset_name == "Breast Cancer":
            if 'data' not in self.cache:
                st.write("Loading Dataset...")
                self.cache['data'] = pd.read_csv("data.csv")
                self.data = self.cache['data']
                self.data_intro()

            columns_without_suffix = [col.replace('_mean', '').replace('_worst', '').replace('_se', '') for col in self.data.columns]
            unique_columns = set(columns_without_suffix)
            st.write("Unique Parameters:", unique_columns)

            

    def data_intro(self):
        st.write("Dataframe first 10 rows: ", self.data.head(10))

        st.write("Shape of Dataset: ", self.data.shape, "  *This dataset consists of 569 samples, each described by 33 features*")
        st.write("Target Value: ", self.data['diagnosis'].value_counts())
        st.write(f"*2 unique values in target column and they are {self.data['diagnosis'].unique()}, their count is {self.data['diagnosis'].value_counts()} respectively*")  

        # Calculate the number of missing values in each column
        missing_values = self.data.isnull().sum()

        # Filter the columns with missing values
        columns_with_missing_values = missing_values[missing_values > 0]

        # Display the columns with missing values and the count of missing values in each column
        if not columns_with_missing_values.empty:
            st.write("Columns with Missing Values:", columns_with_missing_values)
        else:
            st.write("No missing values in the dataset")

        st.write("Categorical Features:" , self.data.dtypes[self.data.dtypes == "object"])


    
    def data_preprocess_breast_cancer(self):
        st.subheader("Data Preprocessing")
        st.write(f"***Drop Unnecessary Columns:*** id, Unnamed: 32")
        self.data.drop(["id", "Unnamed: 32"], axis=1, inplace=True)
        st.write(self.data.head(5),"*Missing values handled with dropping Unnamed: 32 column*" )

        st.write(f"***Encoding Label:*** diagnosis - Malignant - 1, Benign - 0")
        self.data['diagnosis'] = self.data['diagnosis'].map({'M': 1, 'B':0})
        st.write(self.data["diagnosis"].tail(10))

        self.X = self.data.drop(["diagnosis"], axis=1)
        self.y = self.data["diagnosis"].values

        st.session_state["Plot"] = self.target_value_corelation_plot()
        self.X = (self.X - self.X.min()) / (self.X.max() - self.X.min())
        st.write("***Normalized Data***: ", self.X.head(10))

    @st.cache_data
    def target_value_corelation_plot(_self):

        tab1, tab2 = st.tabs(["ðŸ—ƒ Matrix", "ðŸ“ˆ Plot"])

        with tab1:
            tab1.subheader("Correlation Matrix ")
            if _self.dataset_name == "Breast Cancer":
                st.subheader("Target Value Corelation Plot")
                # create correlation matrix
                correlation_matrix = _self.data.corr()

                st.write("Correlation Matrix:")
                st.write(correlation_matrix)
                st.write("##### Correlation Matrix Heatmap with Target Values:")
                st.table(correlation_matrix['diagnosis'])
               

        
        with tab2:
            tab2.subheader("Correlation Plot ")

            # Display the correlation matrix heatmap
            st.write("##### Correlation Matrix Heatmap All Features:")
            plt.figure(figsize=(30, 10))
            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
            plt.title('Correlation Matrix Heatmap')
            plt.xticks(rotation=45)
            plt.yticks(rotation=0)
            st.pyplot(plt)

            # Display the correlation matrix heatmap with target values
            diagnosis_correlation = correlation_matrix['diagnosis']

            st.write("##### Correlation Matrix Heatmap with Target Values:")
            plt.figure(figsize=(10, 8))
            sns.heatmap(diagnosis_correlation.to_frame(), annot=True, cmap='coolwarm', fmt=".2f", cbar=False)
            plt.title('Correlation Matrix Heatmap with Target Values')
            plt.xticks(rotation=45)
            plt.yticks(rotation=0)
            st.pyplot(plt)

     # Display scatter plot
        malignant_data = _self.data[_self.data['diagnosis'] == 1]
        benign_data = _self.data[_self.data['diagnosis'] == 0]
        plt.figure(figsize=(8, 6))
        st.write("***Scatter Plot of Radius Mean vs Texture Mean:***")
        # Make scatter plot transparent with less opacity
        sns.scatterplot(x='radius_mean', y='texture_mean', data=malignant_data, label='Malignant', color='red', alpha=0.5)
        sns.scatterplot(x='radius_mean', y='texture_mean', data=benign_data, label='Benign', color='blue', alpha=0.5)
        plt.title('Scatter Plot of Radius Mean vs Texture Mean')
        plt.xlabel('Radius Mean')
        plt.ylabel('Texture Mean')
        plt.legend()

        st.pyplot(plt)

        #postive correlation
        st.write("***Scatter Plot of positive correlation:***")
        fig,ax=plt.subplots(2,2,figsize=(20,25))
        sns.scatterplot(x='perimeter_mean',y='radius_worst',data=_self.data,hue='diagnosis',ax=ax[0][0])
        sns.scatterplot(x='area_mean',y='radius_worst',data=_self.data,hue='diagnosis',ax=ax[1][0])
        sns.scatterplot(x='texture_mean',y='texture_worst',data=_self.data,hue='diagnosis',ax=ax[0][1])
        sns.scatterplot(x='area_worst',y='radius_worst',data=_self.data,hue='diagnosis',ax=ax[1][1])
        st.pyplot(fig)       

        #negative correlation
        st.write("***Scatter Plot of negative correlation:***")
        fig,ax=plt.subplots(2,2,figsize=(20,25))
        sns.scatterplot(x='area_mean',y='fractal_dimension_mean',data=_self.data,hue='diagnosis',ax=ax[0][0])
        sns.scatterplot(x='radius_mean',y='smoothness_se',data=_self.data,hue='diagnosis',ax=ax[1][0])
        sns.scatterplot(x='smoothness_se',y='perimeter_mean',data=_self.data,hue='diagnosis',ax=ax[0][1])
        sns.scatterplot(x='area_mean',y='smoothness_se',data=_self.data,hue='diagnosis',ax=ax[1][1])
        st.pyplot(fig)

    
    def parameter_tuning(self):
        st.subheader("**Model**")
        st.write(f"***Classifiers*** = {self.classifier_name}")
        
        #### HYPERPARAMETER TUNING ####
        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)

        if self.classifier_name == "KNN":
            kf=KFold(n_splits=5,shuffle=True,random_state=42)
            parameter={'n_neighbors': np.arange(1, 30, 1)}
            knn=KNeighborsClassifier()
            knn_cv=GridSearchCV(knn, param_grid=parameter, cv=kf, verbose=1)
            knn_cv.fit(X_train, y_train)
            st.write("best parameter values: ", knn_cv.best_params_)
            self.best_param = knn_cv.best_params_["n_neighbors"]

        elif self.classifier_name == "SVM":
            param_grid = {'C': np.arange(1, 100, 1), 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'linear']}
            grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
            grid.fit(X_train, y_train)
            st.write("best parameter values: ", grid.best_params_)
            self.best_param = grid.best_params_["C"]    

        elif self.classifier_name == "Naive Bayes(GaussianNB)":
            params_NB = {'var_smoothing': np.logspace(0,-9, num=10)}
            gs_NB = GridSearchCV(estimator=GaussianNB(), 
                            param_grid=params_NB, 
                            cv=KFold(n_splits=5),
                            verbose=1, 
                            scoring='accuracy') 
            gs_NB.fit(X_train, y_train)
            st.write("best parameter values: ", gs_NB.best_params_)
            self.best_param = gs_NB.best_params_["var_smoothing"]


    def add_parameter_ui(self):
        if self.classifier_name == "SVM":
            C = st.sidebar.slider("C", 1, 100,step=1, value= self.best_param)
            self.params["C"] = C
            kernel = st.sidebar.radio("kernel", ("rbf", "poly", "linear")) 
            self.params["kernel"] = kernel

        elif self.classifier_name == "KNN":
            n_neighbors = st.sidebar.slider("n_neighbors", 1, 15, value= self.best_param)
            self.params["n_neighbors"] = n_neighbors

        elif self.classifier_name == "Naive Bayes(GaussianNB)":
            var_smoothing = st.sidebar.slider("var_smoothing", 0.01, 5.0, value= self.best_param)
            self.params["var_smoothing"] = var_smoothing
        


    def models(self):
        self.get_classifier()
        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)

        self.clf.fit(X_train, y_train)
        y_pred = self.clf.predict(X_test)

        acc = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)

        st.write(f"Accuracy = {acc}")
        st.write(f"F1 Score = {f1}")
        st.write(f"Precision = {precision}")
        st.write(f"Recall = {recall}")

        # Define the categories for confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        st.write("***Confusion Matrix:***")
        st.write(cm)
        
        #plot 
        f, ax =plt.subplots(figsize = (5,5))

        sns.heatmap(cm,annot = True, linewidths= 0.5, linecolor="red", fmt=".0f", ax=ax)
        plt.xlabel("y_pred")
        plt.ylabel("y_true")
        st.pyplot(f)  


    def get_classifier(self):
        if self.classifier_name == "KNN":
            self.clf = KNeighborsClassifier(n_neighbors=self.params["n_neighbors"])
        
        elif self.classifier_name == "SVM":
            self.clf = SVC(C=self.params["C"])
        
        # GaussianNB : Tahmin edeceÄŸiniz veri veya kolon sÃ¼rekli (real,ondalÄ±klÄ± vs.) ise
        # BernoulliNB : Tahmin edeceÄŸiniz veri veya kolon ikili ise ( Evet/HayÄ±r , Sigara iÃ§iyor/ Ä°Ã§miyor vs.)??? daha kÃ¶tÃ¼ sonuÃ§ veriyor
        # MultinomialNB : Tahmin edeceÄŸiniz veri veya kolon nominal ise ( Int sayÄ±lar )
        
        elif self.classifier_name == "Naive Bayes(GaussianNB)":
            self.clf = GaussianNB(var_smoothing=self.params["var_smoothing"])